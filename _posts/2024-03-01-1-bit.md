---
title: "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"
categories:
  - Blog
---
The paper introduces BitNet b1.58, a 1.58-bit large language model (LLM) variant that matches the performance of full-precision models while being much more cost-effective for inference.

## Method Overview

BitNet b1.58 constrains all parameters to three values: -1, 0 or +1 (1.58-bit). In this way, the usual operations only require additions and no multiplications as shown below:


![](https://media.licdn.com/dms/image/D4E12AQH3oYjl-ikVCg/article-inline_image-shrink_1000_1488/0/1709330879953?e=1715212800&v=beta&t=GbowcxmNbD3u-g67QXYfViOq854AMAjmY2BZw4Z67bw)

  

It's using an _absmean_ quantization function that first scales the weight matrix by its average absolute value, and then round each value to the nearest integer among {-1, 0, +1}.

The final model adopts the LLaMA architecture with replaced linear layers by BitLinear (as described in [BitNet](https://arxiv.org/pdf/2310.11453.pdf)). It is trained from scratch with 1.58-bit weights and 8-bit activations. Because of this, the model requires "almost no multiplications".

## Results

The key results of BitNet b1.58 are:

- BitNet b1.58 matches perplexity and end-task performance of full-precision LLaMA LM baselines from 3B parameters.

![](https://media.licdn.com/dms/image/D4E12AQF7Yx_VUolcxw/article-inline_image-shrink_400_744/0/1709331173086?e=1715212800&v=beta&t=oexUCC4-dUboZqicaP1pH5CbdzeJAD2zLTbpFCBk0wA)


- At 3.9B parameters, BitNet b1.58 outperforms the 3B LLaMA LM while using 3.3x less memory and being 2.4x faster.

![](https://media.licdn.com/dms/image/D4E12AQHN4xXR7zA55Q/article-inline_image-shrink_400_744/0/1709331147704?e=1715212800&v=beta&t=b9dn9lE0UH0NWfhgVwSOUJcdKoz_7X6a8Zpvqb5IBfk)


- BitNet b1.58 is significantly more energy-efficient, with up to 41x lower energy consumption than full-precision models.

![](https://media.licdn.com/dms/image/D4E12AQHtuQvY-XMFgg/article-inline_image-shrink_400_744/0/1709331239975?e=1715212800&v=beta&t=ara8SAu9wz7aDYcMBCe2itJC9vFtI681q5amYiRs8as)

  

- It also demonstrates up to 11x higher batch size and 8.9x higher throughput compared to baselines.

![](https://media.licdn.com/dms/image/D4E12AQHsUM62vJHmfg/article-inline_image-shrink_400_744/0/1709331291284?e=1715212800&v=beta&t=aP11_MXye7m_yCjIT906wNpoV3pqz4ZzmotRRW5kasM)

- It enables the creation of new hardware specifically designed to take advantage of this setup.

## Conclusion

The paper introduces BitNet b1.58 which uses a ternary representation {-1, 0 or +1} for all parameters of the LLM. This shows very promising results for 3B models where it matches the performance while being around two times faster and requiring 3 times less memory. More details in [the paper](https://huggingface.co/papers/2402.17764).

Congrats to the authors for their work!

Ma, Shuming et al. “The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits.” (2024).
