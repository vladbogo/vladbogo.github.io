---
title: "EMO: Emote Portrait Alive"
categories:
  - Blog
---
Today's paper introduces EMO, a new framework for generating realistic talking head videos from just a single portrait photo and an audio clip. The key innovation lies in EMO's ability to directly translate audio cues like tone and pronunciation into corresponding facial expressions and head movements. By capturing the dynamic link between vocals and motions, EMO can animate portrait photos with a diverse range of lively and natural motions synchronized to the audio.

### Method Overview

EMO receives a single reference image that depitcs a portrait of a character and generates a video synchronized with an input audio that preserves the natural head motion and shows vivid face experssions that are in harmony with the audio.

EMO has two stages: in the first stage, a Reference Net is used to extract _frame encodings_ from the reference image and the previous generated frames. In the second stage, a _diffusion process_ is used. It uses a Backbone Network that serves as the video generation engine. It uses two attention mechanisms: Reference Attention to preserve the identity of the person in the input photo and Audio Attention to modulate the facial expressions and head poses based on the embedded audio features.


![](https://media.licdn.com/dms/image/D5612AQHnCBfls8waTQ/article-inline_image-shrink_1000_1488/0/1709162262497?e=1714608000&v=beta&t=wiO64NOC0GWVAa0dXRNj7Op8R64DH5yr_o3AAYZu6EY)

  

Unlike other approaches that rely on 3D face models or landmarks as intermediate constraints, EMO directly generates the output video frames. The authors collect a 250 hour video dataset covering diverse facial movements like speeches, conversations, and singing in multiple languages. They combine this data with two existing datasets (HDTF and VFHQ) and then train the model.

### Results

Evaluations demonstrate EMO's ability to produce videos not just with accurate lip sync, but also emotionally varied facial expressions in line with the audio tone achieving good results and surpassing prior works:

![](https://media.licdn.com/dms/image/D5612AQGb_WDSuzKjiw/article-inline_image-shrink_400_744/0/1709163026478?e=1714608000&v=beta&t=9B_-tYoo89aCVb9u9dft5gHEf4fM-tXYrCyrDkc-hAc)

  
Here are some qualitative results:


![](https://media.licdn.com/dms/image/D5612AQFfLYKjs84c9Q/article-inline_image-shrink_1000_1488/0/1709163359196?e=1714608000&v=beta&t=BbrjvT5c_sy75U77VQF_UmMivQOXWzhLoJFFsto_NGU)

  

### Conclusion

EMO framework significantly pushes the bar on creating engaging talking head videos from audio, unlocking new levels of realism and faithfulness to the vocal style/content. For more details please consults the [full paper](https://huggingface.co/papers/2402.17485) or the [project page](https://humanaigc.github.io/emote-portrait-alive/).

Congrats to the authors for their work!

Tian, Linrui et al. “EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions.” (2024).
