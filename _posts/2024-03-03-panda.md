---
title: "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers"
categories:
  - Blog
---
The paper proposes Panda-70M, a large-scale video dataset consisting of 70 million high-quality video clips paired with textual caption annotations. The key motivation is to create a dataset that can be used for pre-training and can facilitate advanced video understanding. High-quality video-text pairs are valuable for pretraining but difficult and expensive to manually collect at scale. Hence, the authors develop an automatic pipeline to generate captions by leveraging multiple teacher models and multiple input modalities. The new datasets, Panda-70M is then used for pre-training and the models are tested on three downstream tasks: video captioning, text-video retrieval and text-driven video generation.

## Method Overview

The process starts by taking 3.8M videos from the publicly available dataset _HD-VILA-100M._ These are then split into semantically consistent video clips. In order to achieve this, they use a method that balances semantic coherence within a clip while keeping clips sufficiently long. The authors then use multiple (8) cross-modality teacher models to generate captions for each clip. Now, for each clips there are multiple candidate captions. The authors use a fine-tuned retrieval model to select the best caption. They fine-tune an existing retrieval model on a subset of clips where they have annotated the best caption manually.

![](https://media.licdn.com/dms/image/D4E12AQFA44pc0JnONg/article-inline_image-shrink_400_744/0/1709507119466?e=1716422400&v=beta&t=bqhsfeO0y1ZCGPm_trEjQ9hNkANk0FyCzYSFvoAOCgA)

Finally, because running 8 different captioning models is computational intensive, in order to possibly further scale this dataset, the authors train a student captioning model on Panda-70M (obtained using 8 models) to learn from the collection of 8 students. The architecture of this student is shown below:

![](https://media.licdn.com/dms/image/D4E12AQH4ZPmBfTZ3iA/article-inline_image-shrink_1000_1488/0/1709507119459?e=1716422400&v=beta&t=fvomzH0jTNOieb0wymNRRgKWIhvz5HE6bj3huydLUBA)
## Results

The constructed Panda-70M dataset contains 70.8 million 720p video clips with an average duration of 8.5 seconds paired with 13.2 word captions. Models pretrained on this dataset demonstrate significant gains over baselines on downstream tasks like:

- video captioning

![](https://media.licdn.com/dms/image/D4E12AQF4N14EPkRp2g/article-inline_image-shrink_400_744/0/1709507150422?e=1716422400&v=beta&t=Io4jc0PaGKYREggmmyWMXwQlxcyujMl4Iz-ctSBvSYk)

- text/video retrieval

![](https://media.licdn.com/dms/image/D4E12AQEIrru0h2ELoQ/article-inline_image-shrink_400_744/0/1709507178365?e=1716422400&v=beta&t=IqDxdmZ1D80J8OAPbiKeqbT8K93ebD_logQS6JwYOVs)

- text-to-video generation

![](https://media.licdn.com/dms/image/D4E12AQFiCKmviFGXgw/article-inline_image-shrink_1000_1488/0/1709507196472?e=1716422400&v=beta&t=tSuk7s6i6if3O0a1CXsTwXSIh0KJuwDeR-1iRkdDCrY)
Also, qualitatively, the results look very promising as shown below:

![](https://media.licdn.com/dms/image/D4E12AQGwUgVYvcL4lg/article-inline_image-shrink_1000_1488/0/1709507119608?e=1716422400&v=beta&t=lwdyKlIObHB_4EOdNgH3tC5dt64GQPscRl8OV7REG7w)

![](https://media.licdn.com/dms/image/D4E12AQHOtB3tgJmSfA/article-inline_image-shrink_1000_1488/0/1709507119436?e=1716422400&v=beta&t=7p-ITNHpKFu1_6TIsKRyyQok0pdCblW0ZVcbvR6yDF4)
## Conclusion

In this paper, the authors introduce Panda-70M, a large-scale, high-quality video dataset. Pretraining on this dataset is shown to benefit several video and language tasks. More details in the [full paper](https://huggingface.co/papers/2402.19479).

Congrats to the authors for their work!

Project page: [https://snap-research.github.io/Panda-70M/](https://snap-research.github.io/Panda-70M/)

Code: [https://github.com/snap-research/Panda-70M](https://github.com/snap-research/Panda-70M)

Chen, Tsai-Shien et al. “Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers.” (2024).
