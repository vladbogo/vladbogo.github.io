---
title: "Semantic Evaluation of LLMs"
categories:
  - Blog
---
In the ever-evolving landscape of Large Language Models (LLMs), the need for scalable and accurate evaluation metrics is more pressing than ever. In this blog, we will talk about SemScore, a new evaluation metric that leverages Semantic Textual Similarity (STS) to offer a direct comparison between model outputs and gold standard responses. This approach not only promises to streamline the assessment of instruction-tuned LLMs but also aligns closely with human judgment, marking a significant step forward in automated LLM evaluation.

## The SemScore Advantage

SemScore aims to address the limitations of existing evaluation metrics such as BLEU and ROUGE, which often struggle with the nuanced outputs of instruction-tuned models. To give a better idea, in the below example, while the LLM response is rated by a human evaluator as very high quality, the BLEU and ROUGE scores are low because there is a low N-gram overlap between the generation and the expected target response.

![](https://media.licdn.com/dms/image/D4E12AQERCT7pjko1dA/article-inline_image-shrink_400_744/0/1707060209167?e=1712793600&v=beta&t=Y5wRPBA1CsIt4m0kufy3HfI1fdMS5jWj9euHCY9jcTU)

Limitations of current evaluation metrics such as BLEU and ROUGE

The idea behind SemScore is simple yet effective: compute the cosine similarity between the embedded generation and target.

## A Comparative Study

In order to better understand the effectiveness of the proposed method, the authors first perform a human study that ranks the most popular LLMs in order of their performance.

![](https://media.licdn.com/dms/image/D4E12AQGqBjswm7jAIg/article-inline_image-shrink_400_744/0/1707060991077?e=1712793600&v=beta&t=FFluRS9J2mrcYMpne66KTlyeRFt1YslDwayubm82WEE)

Human evaluation of LLMs

Based on the human evaluation from above, a human ranking of LLMs is derived that stands as a basis for comparison with other evaluation metrics including SemScore:

![](https://media.licdn.com/dms/image/D4E12AQGRgvMHqailuw/article-inline_image-shrink_400_744/0/1707061113716?e=1712793600&v=beta&t=qR7xZkO9g662CYl3mgkdyuudm_P1l8G3KcQPfskNsfw)

Model ranking according to different metrics

Based on these results SemScore and G-Eval-4 have the strongest correlation with the human judgement. G-Eval-4 uses GPT-4 as a judge and while it correlates with the human judgement, may have larger costs. Nevertheless, both metrics correlate well with the human judgement as shown below:

![](https://media.licdn.com/dms/image/D4E12AQErbeMDdBrtHA/article-inline_image-shrink_400_744/0/1707061718158?e=1712793600&v=beta&t=0Y2hfUD0oULk1Ko7MaNQRQFDBbvkTp8Pe0wCMLZlx3A)

Kendall Ï„ & Pearson r correlation between metrics and human scores

## Conclusion

SemScore represents a very interesting advancement in the evaluation of instruction-tuned LLMs, offering a scalable, efficient, and accurate metric that closely aligns with human judgment. For more details please consult the full paper: [https://arxiv.org/pdf/2401.17072.pdf](https://arxiv.org/pdf/2401.17072.pdf).

Kudos to the authors for their great work!
